{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi uvicorn"
      ],
      "metadata": {
        "id": "hBIcM_2r5VLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmcM7zrxvDlZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from fastapi import FastAPI, HTTPException\n",
        "from pydantic import BaseModel\n",
        "from typing import List\n",
        "import joblib\n",
        "import pandas as pd\n",
        "import logging\n",
        "import traceback\n",
        "\n",
        "# Load the trained AdaBoost model and label encoder from separate .pkl files\n",
        "current_dir = os.getcwd()\n",
        "model_path = os.path.join(current_dir, \"movement_detection_adaboost_model.pkl\")  # AdaBoost model\n",
        "label_encoder_path = os.path.join(current_dir, \"movement_detection_adaboost_model_label_encoder.pkl\")  # Label encoder\n",
        "scaler_path = os.path.join(current_dir, \"scaler.pkl\")  # Scaler\n",
        "\n",
        "model = joblib.load(model_path)  # Load AdaBoost model\n",
        "label_encoder = joblib.load(label_encoder_path)  # Load label encoder\n",
        "scaler = joblib.load(scaler_path)  # Load scaler\n",
        "\n",
        "# Create FastAPI app\n",
        "app = FastAPI()\n",
        "\n",
        "# Set up logging for better error visibility\n",
        "logging.basicConfig(level=logging.DEBUG)\n",
        "\n",
        "# Define the input schema\n",
        "class TimeSeriesInput(BaseModel):\n",
        "    data: List[List[float]]  # Raw sensor data in a 2D array\n",
        "\n",
        "# Normalize data using the MinMaxScaler\n",
        "def normalize_data(data: pd.DataFrame, scaler) -> pd.DataFrame:\n",
        "    \"\"\"Normalize the test data using the provided scaler.\"\"\"\n",
        "    try:\n",
        "        normalized = scaler.transform(data)\n",
        "        return pd.DataFrame(normalized, columns=data.columns)\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error normalizing data: {str(e)}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Error normalizing data\")\n",
        "\n",
        "# Extract features from a sliding window\n",
        "def extract_features(window: pd.DataFrame) -> dict:\n",
        "    \"\"\"Extract statistical features from a window of data.\"\"\"\n",
        "    features = {}\n",
        "    for col in window.columns:\n",
        "        try:\n",
        "            if len(window[col].unique()) > 1:  # Avoid constant columns\n",
        "                features[f'{col}_mean'] = window[col].mean()\n",
        "                features[f'{col}_std'] = window[col].std()\n",
        "                features[f'{col}_min'] = window[col].min()\n",
        "                features[f'{col}_max'] = window[col].max()\n",
        "            else:\n",
        "                # For constant columns\n",
        "                features[f'{col}_mean'] = window[col].mean()\n",
        "                features[f'{col}_std'] = 0\n",
        "                features[f'{col}_min'] = window[col].min()\n",
        "                features[f'{col}_max'] = window[col].max()\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error extracting feature for column {col}: {str(e)}\")\n",
        "            raise HTTPException(status_code=500, detail=f\"Error extracting feature for column {col}\")\n",
        "    return features\n",
        "\n",
        "@app.post(\"/predict/\")\n",
        "async def predict(timeseries: TimeSeriesInput):\n",
        "    try:\n",
        "        # Convert the input data to a pandas DataFrame\n",
        "        raw_data = pd.DataFrame(timeseries.data, columns=[\n",
        "            \"acc_X\", \"acc_Y\", \"acc_Z\", \"mag_X\", \"mag_Y\", \"mag_Z\", \"gyro_X\", \"gyro_Y\", \"gyro_Z\"\n",
        "        ])\n",
        "\n",
        "        # Validate input dimensions\n",
        "        if len(raw_data) < 50:\n",
        "            raise HTTPException(status_code=400, detail=\"The input time series must have at least 50 rows.\")\n",
        "\n",
        "        logging.info(f\"Received data with {len(raw_data)} rows.\")\n",
        "\n",
        "        # Normalize the input data before feature extraction\n",
        "        normalized_data = normalize_data(raw_data, scaler)\n",
        "\n",
        "        # Extract features from the window (last 50 rows)\n",
        "        window = normalized_data.iloc[-50:]  # Take the last 50 rows\n",
        "        features = extract_features(window)\n",
        "        features_df = pd.DataFrame([features])  # Convert to DataFrame\n",
        "\n",
        "        logging.info(f\"Extracted features: {features}\")\n",
        "\n",
        "        # Ensure the columns match the ones the model expects\n",
        "        expected_columns = [\n",
        "            \"acc_X_mean\", \"acc_X_std\", \"acc_X_min\", \"acc_X_max\",\n",
        "            \"acc_Y_mean\", \"acc_Y_std\", \"acc_Y_min\", \"acc_Y_max\",\n",
        "            \"acc_Z_mean\", \"acc_Z_std\", \"acc_Z_min\", \"acc_Z_max\",\n",
        "            \"mag_X_mean\", \"mag_X_std\", \"mag_X_min\", \"mag_X_max\",\n",
        "            \"mag_Y_mean\", \"mag_Y_std\", \"mag_Y_min\", \"mag_Y_max\",\n",
        "            \"mag_Z_mean\", \"mag_Z_std\", \"mag_Z_min\", \"mag_Z_max\",\n",
        "            \"gyro_X_mean\", \"gyro_X_std\", \"gyro_X_min\", \"gyro_X_max\",\n",
        "            \"gyro_Y_mean\", \"gyro_Y_std\", \"gyro_Y_min\", \"gyro_Y_max\",\n",
        "            \"gyro_Z_mean\", \"gyro_Z_std\", \"gyro_Z_min\", \"gyro_Z_max\"\n",
        "        ]\n",
        "\n",
        "        # Add any missing columns with default values (to handle the case where a feature is missing)\n",
        "        for col in expected_columns:\n",
        "            if col not in features_df.columns:\n",
        "                features_df[col] = 0\n",
        "\n",
        "        # Ensure the columns are in the correct order\n",
        "        features_df = features_df[expected_columns]\n",
        "\n",
        "        # Predict the label using the trained AdaBoost model\n",
        "        prediction = model.predict(features_df)\n",
        "        predicted_label = label_encoder.inverse_transform(prediction)[0]  # Map numerical prediction to label\n",
        "\n",
        "        # Log the integer prediction to a file\n",
        "        with open(\"predictions.log\", \"a\") as f:\n",
        "            f.write(f\"{predicted_label} \")\n",
        "\n",
        "        logging.info(f\"Prediction: {predicted_label}\")\n",
        "\n",
        "        return {\"predicted_label\": predicted_label}\n",
        "\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error in prediction: {traceback.format_exc()}\")\n",
        "        raise HTTPException(status_code=500, detail=\"Prediction failed\")\n",
        "\n",
        "@app.get(\"/helloworld/\")\n",
        "async def hello_world():\n",
        "    return {\"message\": \"Hello, world!\"}\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import uvicorn\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Swavhm_x5NS6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}